{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import gpytorch\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib widget\n",
    "import numpy as np\n",
    "import sys, os\n",
    "os.environ['MKL_DEBUG_CPU_TYPE'] = '5'\n",
    "sys.path.append('/Users/naji/phd_codebase/psych_model/utils')\n",
    "sys.path.append('/Users/naji/phd_codebase/psych_model/puLearning')\n",
    "sys.path.append('/Users/naji/phd_codebase/psych_model/LPUModels')\n",
    "sys.path.append('/Users/naji/phd_codebase/')\n",
    "\n",
    "\n",
    "import sklearn.model_selection \n",
    "import sklearn.datasets\n",
    "import torch\n",
    "import gpytorch.distributions\n",
    "import lpu.constants as constants\n",
    "import lpu.datasets.animal_no_animal.animal_no_animal_utils as animal_no_animal_utils\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "\n",
    "dtype = constants.DTYPE#torch.float64\n",
    "torch.set_default_dtype(dtype)\n",
    "\n",
    "\n",
    "import math_utils \n",
    "import dataset_utils  \n",
    "import lpu.models.PsychM.psychm as psychm\n",
    "# import visualization\n",
    "\n",
    "    \n",
    "EPSILON = 1e-16\n",
    "COORDINATE_DESCENT = False\n",
    "INDUCING_POINTS_SIZE = 64\n",
    "LEARNINIG_RATE = .005\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "DEVICE = 'cpu'\n",
    "NUM_EPOCHS = 25\n",
    "EPOCH_BLOCKS = 1\n",
    "INTERVAL_LENGTH = 32\n",
    "\n",
    "INTRINSIC_KERNEL_PARAMS = {'normed': False, 'kernel_type': 'laplacian', 'heat_temp': .01, 'noise_factor': .0, \n",
    "                        'amplitude': .5, 'n_neighbor': 5, 'lengthscale': .3, 'neighbor_mode': 'distance',\n",
    "                        'power_factor': 1, 'invert_M_first': False}\n",
    "\n",
    "TRAIN_TEST_RATIO = 0.5\n",
    "DATASET = 'animal_no_animal'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/naji/miniconda3/envs/venv/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/naji/miniconda3/envs/venv/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layers to be extracted are ['classifier_0']\n"
     ]
    }
   ],
   "source": [
    "if DATASET == 'synthetic':\n",
    "    N_SAMPLES = 300\n",
    "    X_y_SAMPLE_TYPE = 'circles'\n",
    "\n",
    "    if X_y_SAMPLE_TYPE == 'circles':\n",
    "        # CIRCLES AS SAMPLE\n",
    "        X, y = sklearn.datasets.make_circles(n_samples=N_SAMPLES, factor=.5,\n",
    "                                                noise=.05)\n",
    "    #     y = 1 - y\n",
    "    elif X_y_SAMPLE_TYPE == 'moons':\n",
    "        X, y = sklearn.datasets.make_moons(n_samples=N_SAMPLES, noise=.05)\n",
    "    elif X_y_SAMPLE_TYPE == 'rbf_kernel':\n",
    "        X = np.random.randn(N_SAMPLES *2).reshape((-1, 2))\n",
    "        phi_X = math_utils.modified_rbf_kernel(X)\n",
    "        alpha = np.random.randn(N_SAMPLES)\n",
    "    l_mean = 0\n",
    "    y_l_mean = 0\n",
    "    y_l_sum = 0\n",
    "    counter = 0\n",
    "    while  y_l_sum < 5 or y_l_sum > 10:\n",
    "        # print (y_l_sum)\n",
    "        sig_X, sig_y, sig_l, real_params = dataset_utils.create_synthetic_dataset(X, y, dim=2, sample_size=0, initial_lambda_range=[0.1, .03], initial_gamma_range=[0.0, .05])\n",
    "        if counter and np.mod(counter, 1000)==0:\n",
    "            print (\"still struggling with dataset_utils.create_synthetic_dataset()... and counter is:\", counter, y_l_sum)\n",
    "        y_l_sum = sig_l[sig_y==1].sum()\n",
    "        l_mean = sig_l.sum()\n",
    "        counter += 1\n",
    "    psych_X, psych_y, psych_l = sig_X, sig_y, sig_l\n",
    "    y = sig_y\n",
    "    l = sig_l\n",
    "elif DATASET == 'swissprot':\n",
    "    X, y, l = dataset_utils.read_swissprot('/Users/naji/Downloads/swissprot.data/', embedding='TF-IDF')\n",
    "    sig_X, sig_y, sig_l = X, y, l\n",
    "    psych_X, psych_y, psych_l = X, y, l\n",
    "elif DATASET == 'animal_no_animal':\n",
    "    output_location = f'{constants.ROOT_PATH}/datasets/animal_no_animal'\n",
    "    subject = 'mta'\n",
    "    model_type = 'vgg'  # or 'HMAX'\n",
    "    layers_to_extract = ['classifier_0']\n",
    "    X, y, l = animal_no_animal_utils.create_animal_no_animal_dataset(output_location=output_location, subject=subject, model_type=model_type, layers_to_extract=layers_to_extract)\n",
    "    sig_X, sig_y, sig_l = X, y, l\n",
    "    psych_X, psych_y, psych_l = X, y, l\n",
    "\n",
    "# Assuming sig_X, psych_X, y, and l are defined somewhere above\n",
    "# encoder = sklearn.preprocessing.LabelEncoder()\n",
    "l_y_cat_transformed = 2 * l.astype(int) + y.astype(int)\n",
    "\n",
    "# Splitting the data\n",
    "train_indices, test_indices, sig_X_train, sig_X_test, psych_X_train, psych_X_test, y_train, y_test, l_train, l_test, l_y_cat_transformed_train, l_y_cat_transformed_test = sklearn.model_selection.train_test_split(\n",
    "    np.arange(len(sig_X)), sig_X, psych_X, y, l, l_y_cat_transformed, stratify=l_y_cat_transformed, test_size=TRAIN_TEST_RATIO, shuffle=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cad94d40cb543c5baee1089f1275e93",
       "version_major": 2,
       "version_minor": 0
      },
      "image/png": "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",
      "text/html": [
       "\n",
       "            <div style=\"display: inline-block;\">\n",
       "                <div class=\"jupyter-widgets widget-label\" style=\"text-align: center;\">\n",
       "                    Figure\n",
       "                </div>\n",
       "                <img src='data:image/png;base64,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' width=640.0/>\n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.hist(l_y_cat_transformed_train, bins=[0, 1 , 2, 3, 4], alpha=0.2)\n",
    "ax.hist(l_y_cat_transformed_test, bins=[0, 1, 2, 3, 4], alpha=0.2)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample = dict()\n",
    "# fig = visualization.plot_all_four(sig_X, l ,y, real_params)\n",
    "# true_gamma, true_lambda, _, _, true_alpha, true_beta = real_params\n",
    "# true_alpha, true_beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, SubsetRandomSampler, TensorDataset\n",
    "# Normalize sig_X_train and psych_X_train, and use the same statistics to normalize sig_X_test and psych_X_test\n",
    "sig_X_mean = sig_X_train.mean(axis=0)\n",
    "sig_X_std = sig_X_train.std(axis=0)\n",
    "psych_X_mean = psych_X_train.mean(axis=0)\n",
    "psych_X_std = psych_X_train.std(axis=0)\n",
    "\n",
    "# Prevent division by zero\n",
    "sig_X_std[sig_X_std == 0] = 1\n",
    "psych_X_std[psych_X_std == 0] = 1\n",
    "\n",
    "# Normalizing data\n",
    "sig_X_train = (sig_X_train - sig_X_mean) / sig_X_std\n",
    "sig_X_test = (sig_X_test - sig_X_mean) / sig_X_std\n",
    "psych_X_train = (psych_X_train - psych_X_mean) / psych_X_std\n",
    "psych_X_test = (psych_X_test - psych_X_mean) / psych_X_std\n",
    "\n",
    "\n",
    "sig_X = (sig_X - sig_X_mean) / sig_X_std\n",
    "psych_X = (psych_X - psych_X_mean) / psych_X_std\n",
    "\n",
    "# Structuring the data\n",
    "data = {\n",
    "    \"full\":\n",
    "        {\"sig_X\": sig_X,\n",
    "        \"psych_X\": psych_X,\n",
    "        \"y\": y,\n",
    "        \"l\": l},\n",
    "\n",
    "    \"train\": {\n",
    "        \"sig_X\": sig_X_train,\n",
    "        \"psych_X\": psych_X_train,\n",
    "        \"y\": y_train,\n",
    "        \"l\": l_train\n",
    "    },\n",
    "    \"test\": {\n",
    "        \"sig_X\": sig_X_test,\n",
    "        \"psych_X\": psych_X_test,\n",
    "        \"y\": y_test,\n",
    "        \"l\": l_test\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Convert all datasets in the dictionary to PyTorch tensors\n",
    "for split in [\"train\", \"test\", \"full\"]:\n",
    "    for key in data[split]:\n",
    "        data[split][key] = torch.tensor(data[split][key], dtype=constants.DTYPE).to(DEVICE)  # Use torch.float32 if your model expects float32\n",
    "\n",
    "# Assuming normalization has been done as per your provided code\n",
    "\n",
    "# Convert all datasets in the dictionary to PyTorch tensors and create DataLoaders\n",
    "dataloaders = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 4096)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = {a\n",
    "#         \"sig_X\": torch.tensor(sig_X),\n",
    "#         \"psych_X\": torch.tensor(psych_X),\n",
    "#         \"y\": torch.tensor(y),\n",
    "#         \"l\": torch.tensor(l)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"SAD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "inducing_points = sklearn.model_selection.train_test_split(data['train']['sig_X'], train_size=INDUCING_POINTS_SIZE, random_state=42, stratify=data['train']['l'])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ElkanGGPC\n",
    "# elkan_ggpc = ElkanGGPC.ElkanGGPC(intrinsic_kernel_params=INTRINSIC_KERNEL_PARAMS)\n",
    "# elkan_ggpc.fit(data['train']['sig_X'], data['train']['l'])\n",
    "# my_fig = visualization.plot_all_four(sig_X_train[:, :2], l_train_train_tensor , y_train, real_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import sklearn.metrics\n",
    "# likelihood.cholesky_factor_a.register_hook(print_grad)\n",
    "# mll =gpytorch.mlls.PredictiveLogLikelihood(likelihood, gp_model, num_data=len(data['train']['sig_X'])).to(DEVICE)\n",
    "# mll = gpytorch.mlls.DeepApproximateMLL(gpytorch.mlls.VariationalELBO(likelihood, gp_model, num_data=600)).to(DEVICE)\n",
    "# def validate(model, likelihood, validation_x, validation_y, validation_l):\n",
    "#     # Set model and likelihood in evaluation mode\n",
    "#     # model.eval()\n",
    "#     # likelihood.eval()\n",
    "#     # X = likelihood.X\n",
    "#     with torch.no_grad():\n",
    "#         validation_output = model(validation_x)\n",
    "#         likelihood.update_input_data(validation_x)\n",
    "#         # Assuming using negative log likelihood as validation metric\n",
    "#         validation_loss = -mll(validation_output, validation_l)# - validation_output.log_prob(validation_y).item()\n",
    "#         torch.where((validation_y > 0) & (validation_y < EPSILON), torch.tensor(EPSILON, device=validation_y.device), validation_y)\n",
    "\n",
    "#         predicted_output = torch.sigmoid(validation_output.mean).cpu().numpy()\n",
    "#         validation_auc = sklearn.metrics.roc_auc_score(validation_y.cpu().numpy(), predicted_output)\n",
    "#     # validation_output.mean\n",
    "#     # Return model to training mode\n",
    "#     # likelihood.update_input_data(X)\n",
    "#     # model.train()\n",
    "#     # likelihood.train()\n",
    "\n",
    "#     return validation_loss, validation_auc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_x = data['test']['sig_X'].to(DEVICE)  # Validation inputs\n",
    "validation_y = data['test']['l'].to(DEVICE)  # Validation labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compute_custom_loss(output, labels):\n",
    "#     # Assuming the likelihood's forward method returns a distribution\n",
    "#     # preds = likelihood(model_output).mean  # Get the mean predictions\n",
    "#     # samples = likelihood.function_samples\n",
    "#     # samples = output.rsample(torch.Size([10]))  # Sample 10 times\n",
    "#     # print(samples.shape, labels.shape)\n",
    "#     # Filter outputs where labels are 1 (positive instances)\n",
    "\n",
    "#     positive_outputs = output.mean[labels == 1]\n",
    "#     # Clamp positive values that are smaller than EPSILON to be EPSILON\n",
    "#     # This leaves negative values and positive values >= EPSILON unchanged\n",
    "#     torch.where((positive_outputs > 0) & (positive_outputs < EPSILON), torch.tensor(EPSILON, device=positive_outputs.device), positive_outputs)\n",
    "\n",
    "#     positive_preds = torch.nn.functional.logsigmoid(positive_outputs)\n",
    "#     custom_loss = positive_preds.mean()  \n",
    "#     # output = output.mean[(labels == 1)]\n",
    "#     # output[output > 1] = 1\n",
    "#     # output[output < EPSILON] = EPSILON\n",
    "#     # positive_preds = torch.nn.functional.logsigmoid()    # print(samples)\n",
    "#     # positive_preds = samples[labels == 1]  # Filter predictions for positive instances\n",
    "#     # print (samples.shape, positive_preds.shape, labels.shape, (labels==1).sum())\n",
    "#     # custom_loss = positive_preds.mean()\n",
    "#     # custom_loss = torch.clamp(custom_loss, min=EPSILON)\n",
    "#     return -custom_loss  # Return the negative log likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Sampler\n",
    "class StratifiedSampler(Sampler):\n",
    "    \"\"\"Stratified Sampling without replacement.\"\"\"\n",
    "    def __init__(self, labels):\n",
    "        self.num_samples = len(labels)\n",
    "        self.class_vector = np.array(labels).astype(int)\n",
    "        self.indices = np.arange(self.num_samples)\n",
    "        \n",
    "    def __iter__(self):\n",
    "        # Find the unique classes and their respective counts\n",
    "        class_counts = np.bincount(self.class_vector)\n",
    "        class_indices = [np.where(self.class_vector == i)[0] for i in range(len(class_counts))]\n",
    "        \n",
    "        # Calculate the number of items per class in each batch\n",
    "        n_batches = self.num_samples // len(class_counts)\n",
    "        indices = []\n",
    "        for _ in range(n_batches):\n",
    "            for class_idx in class_indices:\n",
    "                indices.append(class_idx[np.random.randint(len(class_idx))])\n",
    "        np.random.shuffle(indices)\n",
    "        \n",
    "        # In case the total number of samples isn't divisible by the number of classes,\n",
    "        # randomly select the remaining indices\n",
    "        remainder = self.num_samples - len(indices)\n",
    "        if remainder:\n",
    "            extra_indices = np.random.choice(self.indices, size=remainder, replace=False)\n",
    "            indices = np.concatenate([indices, extra_indices])\n",
    "        \n",
    "        assert len(indices) == self.num_samples\n",
    "        return iter(indices)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = psychmGP()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inducing_points.shape: torch.Size([64, 4096])\n",
      "True alpha:  0\n",
      "True beta:  0\n",
      "Iteration 0: Training loss = 15.321166306415604, Validation loss = None, Validation AUC = None. Elapsed time: 5.477503061294556s,  beta:Parameter containing:\n",
      "tensor(-0.0160, requires_grad=True)\n",
      "Iteration 1: Training loss = 16.571284657432717, Validation loss = None, Validation AUC = None. Elapsed time: 5.231953144073486s,  beta:Parameter containing:\n",
      "tensor(-0.0338, requires_grad=True)\n",
      "Iteration 2: Training loss = 15.058013612421943, Validation loss = None, Validation AUC = None. Elapsed time: 5.375159978866577s,  beta:Parameter containing:\n",
      "tensor(-0.0309, requires_grad=True)\n",
      "Iteration 3: Training loss = 14.590481501192903, Validation loss = None, Validation AUC = None. Elapsed time: 5.3964409828186035s,  beta:Parameter containing:\n",
      "tensor(-0.0370, requires_grad=True)\n",
      "Iteration 4: Training loss = 13.542254718756649, Validation loss = None, Validation AUC = None. Elapsed time: 5.113828897476196s,  beta:Parameter containing:\n",
      "tensor(-0.0383, requires_grad=True)\n",
      "Iteration 5: Training loss = 11.710190324654784, Validation loss = None, Validation AUC = None. Elapsed time: 5.225060224533081s,  beta:Parameter containing:\n",
      "tensor(-0.0285, requires_grad=True)\n",
      "Iteration 6: Training loss = 12.572864826257174, Validation loss = None, Validation AUC = None. Elapsed time: 6.9893951416015625s,  beta:Parameter containing:\n",
      "tensor(-0.0048, requires_grad=True)\n",
      "Iteration 7: Training loss = 9.764560024789377, Validation loss = None, Validation AUC = None. Elapsed time: 5.448429107666016s,  beta:Parameter containing:\n",
      "tensor(0.0091, requires_grad=True)\n",
      "Iteration 8: Training loss = 9.567498617065878, Validation loss = None, Validation AUC = None. Elapsed time: 6.2212231159210205s,  beta:Parameter containing:\n",
      "tensor(0.0190, requires_grad=True)\n",
      "Iteration 9: Training loss = 9.18127633923602, Validation loss = None, Validation AUC = None. Elapsed time: 5.918454170227051s,  beta:Parameter containing:\n",
      "tensor(0.0573, requires_grad=True)\n",
      "Iteration 10: Training loss = 8.89261522325303, Validation loss = None, Validation AUC = None. Elapsed time: 5.858057975769043s,  beta:Parameter containing:\n",
      "tensor(0.0647, requires_grad=True)\n",
      "Iteration 11: Training loss = 11.220398603276438, Validation loss = None, Validation AUC = None. Elapsed time: 5.387399911880493s,  beta:Parameter containing:\n",
      "tensor(0.0662, requires_grad=True)\n",
      "Iteration 12: Training loss = 8.967263462714945, Validation loss = None, Validation AUC = None. Elapsed time: 6.113204002380371s,  beta:Parameter containing:\n",
      "tensor(0.0663, requires_grad=True)\n",
      "Iteration 13: Training loss = 8.60797320968092, Validation loss = None, Validation AUC = None. Elapsed time: 5.499383211135864s,  beta:Parameter containing:\n",
      "tensor(0.0663, requires_grad=True)\n",
      "Iteration 14: Training loss = 9.233821306207275, Validation loss = None, Validation AUC = None. Elapsed time: 5.598986864089966s,  beta:Parameter containing:\n",
      "tensor(0.0663, requires_grad=True)\n",
      "Iteration 15: Training loss = 8.02835735345644, Validation loss = None, Validation AUC = None. Elapsed time: 5.108514070510864s,  beta:Parameter containing:\n",
      "tensor(0.0665, requires_grad=True)\n",
      "Iteration 16: Training loss = 7.8426471878467385, Validation loss = None, Validation AUC = None. Elapsed time: 5.313009977340698s,  beta:Parameter containing:\n",
      "tensor(0.0665, requires_grad=True)\n",
      "Iteration 17: Training loss = 8.112092020275414, Validation loss = None, Validation AUC = None. Elapsed time: 5.171755075454712s,  beta:Parameter containing:\n",
      "tensor(0.0711, requires_grad=True)\n",
      "Iteration 18: Training loss = 7.559771366849002, Validation loss = None, Validation AUC = None. Elapsed time: 5.146491050720215s,  beta:Parameter containing:\n",
      "tensor(0.0874, requires_grad=True)\n",
      "Iteration 19: Training loss = 7.575302602922485, Validation loss = None, Validation AUC = None. Elapsed time: 5.182398080825806s,  beta:Parameter containing:\n",
      "tensor(0.0971, requires_grad=True)\n",
      "Iteration 20: Training loss = 6.725991528603946, Validation loss = None, Validation AUC = None. Elapsed time: 5.20135498046875s,  beta:Parameter containing:\n",
      "tensor(0.0983, requires_grad=True)\n",
      "Iteration 21: Training loss = 6.755724693308625, Validation loss = None, Validation AUC = None. Elapsed time: 5.192653179168701s,  beta:Parameter containing:\n",
      "tensor(0.0980, requires_grad=True)\n",
      "Iteration 22: Training loss = 6.149150444950139, Validation loss = None, Validation AUC = None. Elapsed time: 5.063325881958008s,  beta:Parameter containing:\n",
      "tensor(0.0978, requires_grad=True)\n",
      "Iteration 23: Training loss = 7.0246339812379075, Validation loss = None, Validation AUC = None. Elapsed time: 5.3836071491241455s,  beta:Parameter containing:\n",
      "tensor(0.1002, requires_grad=True)\n",
      "Iteration 24: Training loss = 6.744787103922752, Validation loss = None, Validation AUC = None. Elapsed time: 6.215250730514526s,  beta:Parameter containing:\n",
      "tensor(0.1040, requires_grad=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SKLearnCompatibleGP()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SKLearnCompatibleGP</label><div class=\"sk-toggleable__content\"><pre>SKLearnCompatibleGP()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SKLearnCompatibleGP()"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(sig_X_train, l_y_cat_transformed_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation_x = data['test']['sig_X'].to(DEVICE)  # Validation inputs\n",
    "# validation_y = data['test']['y'].to(DEVICE)  # Validation labels\n",
    "# validation_l = data['test']['l'].to(DEVICE)  # Validation labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.gp_model.eval()\n",
    "GP_clf_score = sklearn.metrics.roc_auc_score(y_test, clf.gp_model(torch.tensor(sig_X_test, dtype=constants.DTYPE)).mean.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.981232"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GP_clf_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_val_ratio = .5\n",
    "# if train_val_ratio:\n",
    "#     # Split data and create DataLoaders for training and validation\n",
    "#     train_indices, val_indices, X_train, X_val, l_train, l_val = sklearn.model_selection.train_test_split(\n",
    "#     np.arange(len(X)), X, l, stratify=l_y_cat_transformed, test_size=train_val_ratio, shuffle=True)\n",
    "#     train_sampler = SubsetRandomSampler(train_indices)\n",
    "#     val_sampler = SubsetRandomSampler(val_indices)\n",
    "# else:\n",
    "#     # Use entire dataset for training\n",
    "#     X_train, l_train = X, l\n",
    "#     train_indices = np.arange(len(X))\n",
    "#     val_indices = []\n",
    "# if not type(X_train) == 'torch.Tensor':\n",
    "#     # Convert X, y to torch tensors\n",
    "#     X_train_tensor = torch.tensor(X_train, dtype=constants.DTYPE).to(DEVICE)\n",
    "#     l_train_tensor = torch.tensor(l_train, dtype=constants.DTYPE).to(DEVICE)\n",
    "\n",
    "# # Create a TensorDataset\n",
    "# dataset = TensorDataset(X_train_tensor, l_train_tensor)\n",
    "\n",
    "\n",
    "# dataloaders = {}\n",
    "# # if no validation set, use the StratifiedSampler\n",
    "# if not train_val_ratio:\n",
    "#     # Create an instance of the StratifiedSampler\n",
    "#     train_sampler = StratifiedSampler(l_train)\n",
    "# else:\n",
    "#     dataloaders[\"val\"] = DataLoader(dataset, batch_size=BATCH_SIZE,  sampler=val_sampler)\n",
    "\n",
    "# dataloaders[\"train\"] = DataLoader(dataset, batch_size=BATCH_SIZE, sampler=train_sampler)\n",
    "\n",
    "# # Initialize model, likelihood, optimizer, and inducing points\n",
    "# X_train_tensor = dataloaders['train'].dataset.tensors[0]  # Assuming the first tensor is your training data\n",
    "# # inducing_points = self._initialize_inducing_points(X_train_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import gpytorch\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "# import time\n",
    "# import copy\n",
    "# import old_matrix_utils\n",
    "# CUSTOM_LOSS_WEIGHT = 0.\n",
    "\n",
    "# inducing_points = torch.tensor(sig_X_train[:INDUCING_POINTS_SIZE], dtype=constants.DTYPE).to(DEVICE)  # For example, take the first 50 data points as inducing points\n",
    "\n",
    "# gp_model = psychm.VariationalGPModel(inducing_points=inducing_points, intrinsic_kernel_params=INTRINSIC_KERNEL_PARAMS).to(DEVICE)\n",
    "# # Use the Adam optimizer\n",
    "# # print (gp_model.)\n",
    "# likelihood = psychm.CustomLikelihood(inducing_points.shape[1], torch.tensor(sig_X_train, dtype=constants.DTYPE),  is_SPM=False).to(DEVICE)\n",
    "# optimizer = torch.optim.Adam([\n",
    "#     {'params': gp_model.parameters()},  # Includes GP hyperparameters\n",
    "#     {'params': likelihood.parameters()},  # Includes parameters a and b\n",
    "# ], lr=LEARNINIG_RATE)#, nesterov=True, momentum=.9, weight_decay=1e-5)\n",
    "# mll =gpytorch.mlls.PredictiveLogLikelihood(likelihood, gp_model, num_data=len(sig_X_train), beta=1).to(DEVICE)\n",
    "\n",
    "# # Use VariationalELBO\n",
    "\n",
    "# # Setup\n",
    "# if DATASET == 'synthetic':\n",
    "#     # Create the mesh grid once\n",
    "#     x1_min, x1_max = data['train']['sig_X'][:, 0].min() - 0.5, data['train']['sig_X'][:, 0].max() + 0.5\n",
    "#     x2_min, x2_max = data['train']['sig_X'][:, 1].min() - 0.5, data['train']['sig_X'][:, 1].max() + 0.5\n",
    "#     xx1, xx2 = torch.meshgrid(torch.linspace(x1_min, x1_max, 50), torch.linspace(x2_min, x2_max, 50))\n",
    "#     grid = torch.stack([xx1.ravel(), xx2.ravel()], dim=1)\n",
    "# lowest_val_loss = float('inf')\n",
    "# lowest_train_loss = float('inf')\n",
    "\n",
    "# # Training loop (simplified for illustration)\n",
    "# model_states = []\n",
    "# validation_predictions = {}\n",
    "# validation_psych_preds = {}\n",
    "# train_loss_list = []\n",
    "# val_loss_list = []\n",
    "# start_time = time.time()\n",
    "# alpha_list = []\n",
    "# epoch_list = []\n",
    "# gamma_list = []\n",
    "# lambda_list = []\n",
    "# beta_list = []\n",
    "# val_auc_list = []\n",
    "\n",
    "# # epoch = 0\n",
    "# start_time = time.time()\n",
    "\n",
    "# for epoch in range(NUM_EPOCHS):\n",
    "#     # val_loss, val_auc = validate(gp_model, likelihood, validation_x, validation_l, validation_y)        \n",
    "#     if DATASET == 'synthetic':\n",
    "#         # Perform validation\n",
    "#         gp_model.eval()\n",
    "#         likelihood.eval()\n",
    "#         with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "#             likelihood.update_input_data(grid)\n",
    "#             predictions = gp_model(grid)\n",
    "#             likelihood(predictions.mean)\n",
    "#             # print(\"F SAMPLES:\", likelihood.function_samples.shape)\n",
    "#             predictions_probs = torch.sigmoid(predictions.mean)\n",
    "#             predicted_probs = predictions_probs.view(xx1.shape)\n",
    "#             psych_predicted_probs = likelihood.psychm_response\n",
    "\n",
    "\n",
    "\n",
    "#             # Store validation predictions\n",
    "#             validation_predictions[epoch] = predicted_probs.cpu().numpy()\n",
    "#             validation_psych_preds[epoch] = psych_predicted_probs.reshape(xx1.shape)\n",
    "#     # gamma_list.append(likelihood.gamma_var.detach().numpy().copy())\n",
    "#     # lambda_list.append(likelihood.lambda_var.detach().numpy().copy())\n",
    "#     gp_model.train()\n",
    "#     likelihood.train()\n",
    "#     mini_batch_counter = 0\n",
    "#     for sig_batch, l_y_batch in dataloaders[\"train\"]:\n",
    "#         l_y_cat_transformed = l_y_batch\n",
    "#         l_batch = l_y_cat_transformed // 2\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         output = gp_model(sig_batch)\n",
    "#         likelihood.update_input_data(sig_batch)  # If needed\n",
    "#         loss = -mll(output, l_batch)\n",
    "#         total_loss = loss\n",
    "\n",
    "#         loss.backward()\n",
    "#         optimizer.step()    \n",
    "\n",
    "#         mini_batch_counter += 1\n",
    "        \n",
    "#     gp_model.eval()\n",
    "#     likelihood.eval()\n",
    "#     # Check if current epoch is within the interval around the EPOCH_BLOCK where validation should be performed\n",
    "    \n",
    "#     if  EPOCH_BLOCKS - INTERVAL_LENGTH // 2 <= epoch % EPOCH_BLOCKS or epoch % EPOCH_BLOCKS <= INTERVAL_LENGTH // 2:\n",
    "#         alpha_list.append(likelihood.variational_mean_alpha.detach().numpy().copy())\n",
    "#         beta_list.append(likelihood.variational_mean_beta.detach().numpy().copy())\n",
    "#         # gamma_list.append(likelihood.gamma_mean.detach().numpy().copy())\n",
    "#         gamma_list.append(likelihood.gamma_mean.detach().numpy().copy())\n",
    "#         lambda_list.append(likelihood.lambda_mean.detach().numpy().copy())\n",
    "#         model_states.append(gp_model.state_dict())\n",
    "#         epoch_list.append(epoch)\n",
    "#         with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "#             validation_output = gp_model(validation_x)\n",
    "#             likelihood.update_input_data(validation_x)\n",
    "#             val_loss = -mll(validation_output, validation_l)# - validation_output.log_prob(validation_y).item()\n",
    "#             # validation_output = validation_output.mean\n",
    "#             predicted_output = torch.sigmoid(validation_output.mean).cpu().numpy()\n",
    "#             val_auc = sklearn.metrics.roc_auc_score(validation_y.cpu().numpy(), predicted_output)\n",
    "        \n",
    "#         val_loss_list.append(val_loss)            \n",
    "#         val_auc_list.append(val_auc)\n",
    "#         if val_loss < lowest_val_loss:\n",
    "#             lowest_val_loss = val_loss\n",
    "#             best_model_state = copy.deepcopy(gp_model.state_dict())\n",
    "\n",
    "#         # Stop training if there is no improvement in validation loss within the specified interval\n",
    "#         if epoch // EPOCH_BLOCKS > 1 and epoch % EPOCH_BLOCKS == INTERVAL_LENGTH // 2:\n",
    "#             improvement = val_loss_list[-INTERVAL_LENGTH] > np.min(val_loss_list[-INTERVAL_LENGTH+1:])\n",
    "#             if not improvement:\n",
    "#                 print(f'Training stopped at epoch {epoch} due to no improvement in validation loss.')\n",
    "#                 break\n",
    "\n",
    "\n",
    "#             # val_loss, val_auc = validate(gp_model, likelihood, validation_x, validation_y, validation_l)\n",
    "#         # print(f\"Iteration {epoch}: Training loss = {total_loss.item()}, Validation loss = {val_loss}, Validation AUC = {val_auc}. Elapsed time: {time.time() - start_time}s,  beta:{likelihood.variational_mean_beta}\")#, alpha:{likelihood.variational_mean_alpha},\")\n",
    "#         train_loss_list.append(total_loss.item())\n",
    "#         start_time = time.time()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.likelihood.gamma.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_idx = np.argmin(val_loss_list)# * EPOCH_BLOCKS  \n",
    "# print (\"The best model is at epoch:\", best_idx)\n",
    "# gamma, lambda_, _ = torch.softmax(torch.tensor([gamma_list[best_idx], lambda_list[best_idx], [0.]]), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.6304, grad_fn=<UnbindBackward0>),\n",
       " tensor(0.2024, grad_fn=<UnbindBackward0>))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.likelihood.gamma, clf.likelihood.lambda_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_gp_model = psychm.VariationalGPModel(inducing_points=inducing_points, intrinsic_kernel_params=INTRINSIC_KERNEL_PARAMS).to(DEVICE)\n",
    "# best_gp_model.load_state_dict(model_states[best_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# likelihood.eval()\n",
    "# likelihood.update_input_data(torch.tensor(sig_X_test, dtype=constants.DTYPE))\n",
    "# likelihood(torch.tensor(validation_output.mean, dtype=constants.DTYPE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "GP_clf_score = sklearn.metrics.roc_auc_score(y_test, clf.gp_model(torch.tensor(sig_X_test, dtype=constants.DTYPE)).mean.detach().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.981232"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GP_clf_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_alpha = torch.tensor(clf.alpha_list[best_idx], dtype=constants.DTYPE).to(DEVICE)\n",
    "# best_beta = torch.tensor(clf.beta_list[best_idx], dtype=constants.DTYPE).to(DEVICE)\n",
    "# best_linear_response = torch.tensor(torch.matmul(best_alpha,  data['test']['sig_X'].t()) + best_beta, dtype=constants.DTYPE).to(DEVICE)\n",
    "# fig, ax = plt.subplots()\n",
    "# diff = torch.sigmoid(best_gp_model(data['test']['sig_X']).mean)[y_test==1].detach().numpy() - ((gamma+ (1-gamma - lambda_) * torch.sigmoid(best_linear_response[y_test==1])) * torch.sigmoid(best_gp_model(data['test']['sig_X']).mean[y_test==1])).detach().numpy()\n",
    "# ax.hist(diff, bins=30, alpha=0.5)\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ((gamma+ (1-gamma - lambda_) * torch.sigmoid(likelihood.linear_response[y_test==1])) * torch.sigmoid(sample[:, y_test==1])).mean() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.sigmoid(clf.likelihood.linear_response[y_test==1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.856"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_test[y_test==1].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8653, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.likelihood.update_input_data(data['test']['sig_X'])\n",
    "clf.likelihood(clf.gp_model(data['test']['sig_X']).mean)\n",
    "gamma, lambda_, _ = torch.softmax(torch.tensor([clf.likelihood.gamma_mean, clf.likelihood.lambda_mean, 0.]), dim=0)\n",
    "linear_response = gamma + (1-gamma - lambda_) * torch.sigmoid(data['test']['sig_X'] @ clf.likelihood.variational_mean_alpha + clf.likelihood.variational_mean_beta)\n",
    "linear_response[y_test == 1].mean()\n",
    "# (linear_response[y_test==1] * torch.sigmoid(linear_response)[y_test==1]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8806, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sigmoid(clf.gp_model(data['test']['sig_X']).mean[y_test==1]).mean() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.428"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_test.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.856"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_test[y_test==1].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.softmax(torch.cat([likelihood.gamma_mean.unsqueeze(0), likelihood.lambda_mean.unsqueeze(0), torch.zeros(1)], dim=0), dim=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 38\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Plot training and validation loss\u001b[39;00m\n\u001b[1;32m     37\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n\u001b[0;32m---> 38\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mclf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_loss_list\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_loss_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTraining Loss\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mlen\u001b[39m(clf\u001b[38;5;241m.\u001b[39mval_loss_list)), clf\u001b[38;5;241m.\u001b[39mval_loss_list, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValidation Loss\u001b[39m\u001b[38;5;124m'\u001b[39m, linestyle\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     40\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/venv/lib/python3.11/site-packages/matplotlib/pyplot.py:3590\u001b[0m, in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3582\u001b[0m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[38;5;241m.\u001b[39mplot)\n\u001b[1;32m   3583\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot\u001b[39m(\n\u001b[1;32m   3584\u001b[0m     \u001b[38;5;241m*\u001b[39margs: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m ArrayLike \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3588\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3589\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[Line2D]:\n\u001b[0;32m-> 3590\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgca\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3591\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3592\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscalex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscalex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3593\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscaley\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscaley\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3594\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3595\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3596\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/venv/lib/python3.11/site-packages/matplotlib/axes/_axes.py:1724\u001b[0m, in \u001b[0;36mAxes.plot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1481\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1482\u001b[0m \u001b[38;5;124;03mPlot y versus x as lines and/or markers.\u001b[39;00m\n\u001b[1;32m   1483\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1721\u001b[0m \u001b[38;5;124;03m(``'green'``) or hex strings (``'#008000'``).\u001b[39;00m\n\u001b[1;32m   1722\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1723\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m cbook\u001b[38;5;241m.\u001b[39mnormalize_kwargs(kwargs, mlines\u001b[38;5;241m.\u001b[39mLine2D)\n\u001b[0;32m-> 1724\u001b[0m lines \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_lines(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39mdata, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)]\n\u001b[1;32m   1725\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m lines:\n\u001b[1;32m   1726\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_line(line)\n",
      "File \u001b[0;32m~/miniconda3/envs/venv/lib/python3.11/site-packages/matplotlib/axes/_base.py:303\u001b[0m, in \u001b[0;36m_process_plot_var_args.__call__\u001b[0;34m(self, axes, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m    301\u001b[0m     this \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    302\u001b[0m     args \u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m--> 303\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_plot_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mambiguous_fmt_datakey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mambiguous_fmt_datakey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/venv/lib/python3.11/site-packages/matplotlib/axes/_base.py:489\u001b[0m, in \u001b[0;36m_process_plot_var_args._plot_args\u001b[0;34m(self, axes, tup, kwargs, return_kwargs, ambiguous_fmt_datakey)\u001b[0m\n\u001b[1;32m    487\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(xy) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m    488\u001b[0m     x \u001b[38;5;241m=\u001b[39m _check_1d(xy[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m--> 489\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43m_check_1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxy\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    491\u001b[0m     x, y \u001b[38;5;241m=\u001b[39m index_of(xy[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[0;32m~/miniconda3/envs/venv/lib/python3.11/site-packages/matplotlib/cbook.py:1358\u001b[0m, in \u001b[0;36m_check_1d\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1352\u001b[0m \u001b[38;5;66;03m# plot requires `shape` and `ndim`.  If passed an\u001b[39;00m\n\u001b[1;32m   1353\u001b[0m \u001b[38;5;66;03m# object that doesn't provide them, then force to numpy array.\u001b[39;00m\n\u001b[1;32m   1354\u001b[0m \u001b[38;5;66;03m# Note this will strip unit information.\u001b[39;00m\n\u001b[1;32m   1355\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshape\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   1356\u001b[0m         \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mndim\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   1357\u001b[0m         \u001b[38;5;28mlen\u001b[39m(x\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m-> 1358\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43matleast_1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1359\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1360\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36matleast_1d\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/venv/lib/python3.11/site-packages/numpy/core/shape_base.py:65\u001b[0m, in \u001b[0;36matleast_1d\u001b[0;34m(*arys)\u001b[0m\n\u001b[1;32m     63\u001b[0m res \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ary \u001b[38;5;129;01min\u001b[39;00m arys:\n\u001b[0;32m---> 65\u001b[0m     ary \u001b[38;5;241m=\u001b[39m asanyarray(ary)\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ary\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     67\u001b[0m         result \u001b[38;5;241m=\u001b[39m ary\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/venv/lib/python3.11/site-packages/torch/_tensor.py:1030\u001b[0m, in \u001b[0;36mTensor.__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m   1028\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[38;5;241m.\u001b[39m__array__, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m   1029\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1030\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1031\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1032\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead."
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9aa29bdc2bf74f92b4a973e441d9d157",
       "version_major": 2,
       "version_minor": 0
      },
      "image/png": "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",
      "text/html": [
       "\n",
       "            <div style=\"display: inline-block;\">\n",
       "                <div class=\"jupyter-widgets widget-label\" style=\"text-align: center;\">\n",
       "                    Figure\n",
       "                </div>\n",
       "                <img src='data:image/png;base64,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' width=1000.0/>\n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if DATASET == 'synthetic':\n",
    "    clf.likelihood.update_input_data(grid)\n",
    "    predictions = clf.gp_model(grid)\n",
    "    predictions_probs = clf.likelihood(predictions.mean)\n",
    "    print(\"STAGE 3\")\n",
    "    predicted_probs = predictions_probs.mean.view(xx1.shape)\n",
    "    # print (likelihood.cholesky_factor_a.grad)\n",
    "    print(\"STAGE 4\")\n",
    "    # psychm_predictions = likelihood(predictions)\n",
    "    print(\"STAGE 5\")\n",
    "    # psychm_predicted_probs = likelihood.modified_term.view(xx1.shape)\n",
    "    # Plot validation predictions in subplots\n",
    "    num_plots = len([epoch for epoch in validation_predictions.keys() if epoch % EPOCH_BLOCKS == 0])\n",
    "    cols = 3  # Adjust the number of columns in the subplot grid as needed\n",
    "    rows = num_plots // cols + (num_plots % cols > 0)\n",
    "\n",
    "    plot_index = 1\n",
    "\n",
    "    plt.figure(figsize=(14, rows * 4))\n",
    "    for i, (epoch, predicted_probs) in enumerate(validation_predictions.items(), 1):\n",
    "        if epoch % EPOCH_BLOCKS == 0:\n",
    "            plt.subplot(rows, cols, plot_index)\n",
    "            # plt.contourf(xx1.numpy(), xx2.numpy(), validation_psych_preds[epoch], cmap=plt.cm.BrBG, alpha=0.9, extend='both', levels=5)\n",
    "            plt.contourf(xx1.numpy(), xx2.numpy(), predicted_probs, alpha=0.8, cmap=plt.cm.RdYlBu, extend='both', levels=50)\n",
    "            plt.colorbar(label='Probability of Class 1')\n",
    "            plt.title(f'Epoch {epoch} Predictions, Validation Loss: {clf.val_loss_list[i]:.3f}')\n",
    "            # Optional: Add contour lines with hatching for one of the plots\n",
    "            # CS = plt.contour(xx1.numpy(), xx2.numpy(), validation_psych_preds[epoch], levels=5, colors='k', linewidths=1)\n",
    "            plt.clabel(CS, inline=1, fontsize=10)\n",
    "            for collection in CS.collections:\n",
    "                collection.set_hatch('//')  # Set the hatching pattern\n",
    "            plot_index += 1  # Increment plot index for the next subplot                \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(np.arange(len(clf.train_loss_list)), clf.train_loss_list, label='Training Loss')\n",
    "plt.plot(np.arange(len(clf.val_loss_list)), clf.val_loss_list, label='Validation Loss', linestyle='--')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(gp_model.covar_module.M)\n",
    "# plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(gp_model.covar_module.M)\n",
    "# import matplotlib.pyplot as plt\n",
    "# import matplotlib.colors as colors\n",
    "\n",
    "# # Create a custom colormap with 16 different color values\n",
    "# cmap = colors.ListedColormap(['red', 'green', 'blue', 'yellow', 'orange', 'purple', 'cyan', 'magenta',\n",
    "#                               'pink', 'brown', 'gray', 'olive', 'teal', 'navy', 'lime', 'maroon'])\n",
    "\n",
    "# fig = plt.subplot()\n",
    "# plt.imshow(gp_model.covar_module.M[:30, :30], cmap=cmap, vmin=-5, vmax=10)\n",
    "# plt.colorbar()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gamma, lambda_, _, _, alpha, beta = real_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# real_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(likelihood.variational_mean_alpha,\n",
    "# likelihood.variational_mean_beta,\n",
    "# likelihood.gamma, likelihood.lambda_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax  = plt.subplots()\n",
    "# ax.hist(np.asarray(clf.alpha_lis[:, :2]).flatten(), bins=50)\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for array: array is 1-dimensional, but 2 were indexed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[130], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m alpha_arr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray([alpha\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy() \u001b[38;5;28;01mfor\u001b[39;00m alpha \u001b[38;5;129;01min\u001b[39;00m clf\u001b[38;5;241m.\u001b[39malpha_list])\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Plot the trajectory of alpha values\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m ax\u001b[38;5;241m.\u001b[39mplot(\u001b[43malpha_arr\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m, alpha_arr[:, \u001b[38;5;241m1\u001b[39m], label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAlpha Trajectory\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Plot the starting point\u001b[39;00m\n\u001b[1;32m     14\u001b[0m ax\u001b[38;5;241m.\u001b[39mplot(alpha_arr[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m], alpha_arr[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m], marker\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mo\u001b[39m\u001b[38;5;124m'\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStart\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for array: array is 1-dimensional, but 2 were indexed"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb5e670cbbb94688a8522b2b8951dce5",
       "version_major": 2,
       "version_minor": 0
      },
      "image/png": "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",
      "text/html": [
       "\n",
       "            <div style=\"display: inline-block;\">\n",
       "                <div class=\"jupyter-widgets widget-label\" style=\"text-align: center;\">\n",
       "                    Figure\n",
       "                </div>\n",
       "                <img src='data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAoAAAAHgCAYAAAA10dzkAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/H5lhTAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAevklEQVR4nO3df2zfdZ3A8Vfb0W8h0jJvrvtxX9yBh6jAhhurBRfCpWcTyLz9cbEHZpsLgugk3JpTNn6sIrpOBLJEigsTDpOT25QAZ1xTDnsuBqlZ3NYEZYPgwO2MLdvp2jm0Ze3n/rjw9eo6XEf3/Y7v+/FIvn/0zeez7+vLm/J95vP9sYosy7IAACAZlaUeAACA4hKAAACJEYAAAIkRgAAAiRGAAACJEYAAAIkRgAAAiRGAAACJEYAAAIkRgAAAiRGAAACJEYAAAIkRgAAAiRGAAACJEYAAAIkRgAAAiRGAAACJEYAAAIkRgAAAiRGAAACJEYAAAIkRgAAAiRGAAACJEYAAAIkRgAAAiRGAAACJEYAAAIkRgAAAiRGAAACJEYAAAIkRgAAAiRGAAACJEYAAAIkRgAAAiRGAAACJEYAAAIkRgAAAiRGAAACJEYAAAIkRgAAAiRGAAACJEYAAAIkRgAAAiRGAAACJEYAAAIkRgAAAiRGAAACJEYAAAIkRgAAAiRGAAACJEYAAAIkRgAAAiSmbAPzxj38cixcvjlmzZkVFRUU89dRTf/Gcbdu2xYc//OHI5XLxvve9Lx599NFTPicAQKmVTQAeOXIk5s6dGx0dHSd0/CuvvBLXXHNNXHXVVdHb2xv//M//HJ/+9Kfj6aefPsWTAgCUVkWWZVmph5hsFRUV8eSTT8aSJUuOe8ytt94aW7dujZ///OeFtX/6p3+KQ4cORVdXVxGmBAAojbK5AjhRPT090dTUNGatubk5enp6SjQRAEBxTCn1AKXS19cX9fX1Y9bq6+tjcHAw/vCHP8SZZ555zDlDQ0MxNDRU+Hl0dDR++9vfxl/91V9FRUXFKZ8ZAHj7siyLw4cPx6xZs6KyMs1rYckG4Mlob2+Pu+66q9RjAACTYP/+/fHXf/3XpR6jJJINwBkzZkR/f/+Ytf7+/qitrR336l9ExJo1a6K1tbXw88DAQJx77rmxf//+qK2tPaXzAgCTY3BwMPL5fJx99tmlHqVkkg3AxsbG6OzsHLP2zDPPRGNj43HPyeVykcvljlmvra0VgADwDpPy27fK5oXv3//+99Hb2xu9vb0R8X9f89Lb2xv79u2LiP+7erds2bLC8TfddFPs3bs3vvjFL8aePXviwQcfjO9+97uxatWqUowPAFA0ZROAP/vZz+LSSy+NSy+9NCIiWltb49JLL421a9dGRMRvfvObQgxGRPzN3/xNbN26NZ555pmYO3du3HffffGtb30rmpubSzI/AECxlOX3ABbL4OBg1NXVxcDAgJeAAeAdwvN3GV0BBADgxAhAAIDECEAAgMQIQACAxAhAAIDECEAAgMQIQACAxAhAAIDECEAAgMQIQACAxAhAAIDECEAAgMQIQACAxAhAAIDECEAAgMQIQACAxAhAAIDECEAAgMQIQACAxAhAAIDECEAAgMQIQACAxAhAAIDECEAAgMQIQACAxAhAAIDECEAAgMQIQACAxAhAAIDECEAAgMQIQACAxAhAAIDECEAAgMQIQACAxAhAAIDECEAAgMQIQACAxAhAAIDECEAAgMQIQACAxAhAAIDECEAAgMQIQACAxAhAAIDECEAAgMQIQACAxAhAAIDECEAAgMQIQACAxAhAAIDECEAAgMQIQACAxAhAAIDECEAAgMQIQACAxAhAAIDECEAAgMQIQACAxAhAAIDECEAAgMQIQACAxAhAAIDECEAAgMSUVQB2dHTEnDlzoqamJhoaGmL79u1vefyGDRvi/e9/f5x55pmRz+dj1apV8cc//rFI0wIAlEbZBOCWLVuitbU12traYufOnTF37txobm6O1157bdzjH3vssVi9enW0tbXF7t274+GHH44tW7bEbbfdVuTJAQCKq2wC8P77748bbrghVqxYER/84Adj48aNcdZZZ8Ujjzwy7vHPPfdcXHHFFXHdddfFnDlz4mMf+1hce+21f/GqIQDAO11ZBODw8HDs2LEjmpqaCmuVlZXR1NQUPT09455z+eWXx44dOwrBt3fv3ujs7Iyrr766KDMDAJTKlFIPMBkOHjwYIyMjUV9fP2a9vr4+9uzZM+451113XRw8eDA++tGPRpZlcfTo0bjpppve8iXgoaGhGBoaKvw8ODg4OQ8AAKCIyuIK4MnYtm1brFu3Lh588MHYuXNnPPHEE7F169a4++67j3tOe3t71NXVFW75fL6IEwMATI6KLMuyUg/xdg0PD8dZZ50Vjz/+eCxZsqSwvnz58jh06FD8x3/8xzHnLFq0KD7ykY/E17/+9cLav/3bv8WNN94Yv//976Oy8tg2Hu8KYD6fj4GBgaitrZ3cBwUAnBKDg4NRV1eX9PN3WVwBrK6ujvnz50d3d3dhbXR0NLq7u6OxsXHcc15//fVjIq+qqioiIo7XxLlcLmpra8fcAADeacriPYAREa2trbF8+fJYsGBBLFy4MDZs2BBHjhyJFStWRETEsmXLYvbs2dHe3h4REYsXL477778/Lr300mhoaIiXX3457rzzzli8eHEhBAEAylHZBGBLS0scOHAg1q5dG319fTFv3rzo6uoqfDBk3759Y6743XHHHVFRURF33HFH/PrXv473vOc9sXjx4vjqV79aqocAAFAUZfEewFLxHgIAeOfx/F0m7wEEAODECUAAgMQIQACAxAhAAIDECEAAgMQIQACAxAhAAIDECEAAgMQIQACAxAhAAIDECEAAgMQIQACAxAhAAIDECEAAgMQIQACAxAhAAIDECEAAgMQIQACAxAhAAIDECEAAgMQIQACAxAhAAIDECEAAgMQIQACAxAhAAIDECEAAgMQIQACAxAhAAIDECEAAgMQIQACAxAhAAIDECEAAgMQIQACAxAhAAIDECEAAgMQIQACAxAhAAIDECEAAgMQIQACAxAhAAIDECEAAgMQIQACAxAhAAIDECEAAgMQIQACAxAhAAIDECEAAgMQIQACAxAhAAIDECEAAgMQIQACAxAhAAIDECEAAgMQIQACAxAhAAIDECEAAgMQIQACAxAhAAIDECEAAgMQIQACAxAhAAIDECEAAgMQIQACAxJRVAHZ0dMScOXOipqYmGhoaYvv27W95/KFDh2LlypUxc+bMyOVyccEFF0RnZ2eRpgUAKI0ppR5gsmzZsiVaW1tj48aN0dDQEBs2bIjm5uZ48cUXY/r06cccPzw8HH//938f06dPj8cffzxmz54dv/rVr+Kcc84p/vAAAEVUkWVZVuohJkNDQ0Ncdtll8cADD0RExOjoaOTz+bj55ptj9erVxxy/cePG+PrXvx579uyJM84446Tuc3BwMOrq6mJgYCBqa2vf1vwAQHF4/i6Tl4CHh4djx44d0dTUVFirrKyMpqam6OnpGfec73//+9HY2BgrV66M+vr6uOiii2LdunUxMjJy3PsZGhqKwcHBMTcAgHeasgjAgwcPxsjISNTX149Zr6+vj76+vnHP2bt3bzz++OMxMjISnZ2dceedd8Z9990XX/nKV457P+3t7VFXV1e45fP5SX0cAADFUBYBeDJGR0dj+vTp8dBDD8X8+fOjpaUlbr/99ti4ceNxz1mzZk0MDAwUbvv37y/ixAAAk6MsPgQybdq0qKqqiv7+/jHr/f39MWPGjHHPmTlzZpxxxhlRVVVVWPvABz4QfX19MTw8HNXV1ceck8vlIpfLTe7wAABFVhZXAKurq2P+/PnR3d1dWBsdHY3u7u5obGwc95wrrrgiXn755RgdHS2svfTSSzFz5sxx4w8AoFyURQBGRLS2tsamTZvi29/+duzevTs++9nPxpEjR2LFihUREbFs2bJYs2ZN4fjPfvaz8dvf/jZuueWWeOmll2Lr1q2xbt26WLlyZakeAgBAUZTFS8ARES0tLXHgwIFYu3Zt9PX1xbx586Krq6vwwZB9+/ZFZeWfejefz8fTTz8dq1atiksuuSRmz54dt9xyS9x6662leggAAEVRNt8DWAq+RwgA3nk8f5fRS8AAAJwYAQgAkBgBCACQGAEIAJAYAQgAkBgBCACQGAEIAJAYAQgAkBgBCACQGAEIAJAYAQgAkBgBCACQGAEIAJAYAQgAkBgBCACQGAEIAJAYAQgAkBgBCACQGAEIAJAYAQgAkBgBCACQGAEIAJAYAQgAkBgBCACQGAEIAJAYAQgAkBgBCACQGAEIAJAYAQgAkBgBCACQGAEIAJAYAQgAkBgBCACQGAEIAJAYAQgAkBgBCACQGAEIAJAYAQgAkBgBCACQGAEIAJAYAQgAkBgBCACQGAEIAJAYAQgAkBgBCACQGAEIAJAYAQgAkBgBCACQGAEIAJAYAQgAkBgBCACQGAEIAJAYAQgAkBgBCACQGAEIAJAYAQgAkBgBCACQGAEIAJAYAQgAkBgBCACQGAEIAJAYAQgAkBgBCACQmLIKwI6OjpgzZ07U1NREQ0NDbN++/YTO27x5c1RUVMSSJUtO7YAAAKeBsgnALVu2RGtra7S1tcXOnTtj7ty50dzcHK+99tpbnvfqq6/Gv/zLv8SiRYuKNCkAQGmVTQDef//9ccMNN8SKFSvigx/8YGzcuDHOOuuseOSRR457zsjISHzyk5+Mu+66K84777wiTgsAUDplEYDDw8OxY8eOaGpqKqxVVlZGU1NT9PT0HPe8L3/5yzF9+vS4/vrrT+h+hoaGYnBwcMwNAOCdpiwC8ODBgzEyMhL19fVj1uvr66Ovr2/cc5599tl4+OGHY9OmTSd8P+3t7VFXV1e45fP5tzU3AEAplEUATtThw4dj6dKlsWnTppg2bdoJn7dmzZoYGBgo3Pbv338KpwQAODWmlHqAyTBt2rSoqqqK/v7+Mev9/f0xY8aMY47/5S9/Ga+++mosXry4sDY6OhoREVOmTIkXX3wxzj///GPOy+VykcvlJnl6AIDiKosrgNXV1TF//vzo7u4urI2OjkZ3d3c0NjYec/yFF14Yzz//fPT29hZuH//4x+Oqq66K3t5eL+0CAGWtLK4ARkS0trbG8uXLY8GCBbFw4cLYsGFDHDlyJFasWBEREcuWLYvZs2dHe3t71NTUxEUXXTTm/HPOOSci4ph1AIByUzYB2NLSEgcOHIi1a9dGX19fzJs3L7q6ugofDNm3b19UVpbFBU8AgLelIsuyrNRDvFMNDg5GXV1dDAwMRG1tbanHAQBOgOfvMnkPIAAAJ04AAgAkRgACACRGAAIAJEYAAgAkRgACACRGAAIAJEYAAgAkRgACACRGAAIAJEYAAgAkRgACACRGAAIAJEYAAgAkRgACACRGAAIAJEYAAgAkRgACACRGAAIAJEYAAgAkRgACACRGAAIAJEYAAgAkRgACACRGAAIAJEYAAgAkRgACACRGAAIAJEYAAgAkRgACACRGAAIAJEYAAgAkRgACACRGAAIAJEYAAgAkRgACACRGAAIAJEYAAgAkRgACACRGAAIAJEYAAgAkRgACACRGAAIAJEYAAgAkRgACACRGAAIAJEYAAgAkRgACACRGAAIAJEYAAgAkRgACACRGAAIAJEYAAgAkRgACACRGAAIAJEYAAgAkRgACACRGAAIAJEYAAgAkRgACACRGAAIAJEYAAgAkRgACACSmrAKwo6Mj5syZEzU1NdHQ0BDbt28/7rGbNm2KRYsWxdSpU2Pq1KnR1NT0lscDAJSLsgnALVu2RGtra7S1tcXOnTtj7ty50dzcHK+99tq4x2/bti2uvfba+NGPfhQ9PT2Rz+fjYx/7WPz6178u8uQAAMVVkWVZVuohJkNDQ0Ncdtll8cADD0RExOjoaOTz+bj55ptj9erVf/H8kZGRmDp1ajzwwAOxbNmyE7rPwcHBqKuri4GBgaitrX1b8wMAxeH5u0yuAA4PD8eOHTuiqampsFZZWRlNTU3R09NzQn/G66+/Hm+88Ua8+93vPu4xQ0NDMTg4OOYGAPBOUxYBePDgwRgZGYn6+vox6/X19dHX13dCf8att94as2bNGhORf669vT3q6uoKt3w+/7bmBgAohbIIwLdr/fr1sXnz5njyySejpqbmuMetWbMmBgYGCrf9+/cXcUoAgMkxpdQDTIZp06ZFVVVV9Pf3j1nv7++PGTNmvOW59957b6xfvz5++MMfxiWXXPKWx+Zyucjlcm97XgCAUiqLK4DV1dUxf/786O7uLqyNjo5Gd3d3NDY2Hve8e+65J+6+++7o6uqKBQsWFGNUAICSK4srgBERra2tsXz58liwYEEsXLgwNmzYEEeOHIkVK1ZERMSyZcti9uzZ0d7eHhERX/va12Lt2rXx2GOPxZw5cwrvFXzXu94V73rXu0r2OAAATrWyCcCWlpY4cOBArF27Nvr6+mLevHnR1dVV+GDIvn37orLyTxc8v/nNb8bw8HD84z/+45g/p62tLb70pS8Vc3QAgKIqm+8BLAXfIwQA7zyev8vkPYAAAJw4AQgAkBgBCACQGAEIAJAYAQgAkBgBCACQGAEIAJAYAQgAkBgBCACQGAEIAJAYAQgAkBgBCACQGAEIAJAYAQgAkBgBCACQGAEIAJAYAQgAkBgBCACQGAEIAJAYAQgAkBgBCACQGAEIAJAYAQgAkBgBCACQGAEIAJAYAQgAkBgBCACQGAEIAJAYAQgAkBgBCACQGAEIAJAYAQgAkBgBCACQGAEIAJAYAQgAkBgBCACQGAEIAJAYAQgAkBgBCACQGAEIAJAYAQgAkBgBCACQGAEIAJAYAQgAkBgBCACQGAEIAJAYAQgAkBgBCACQGAEIAJAYAQgAkBgBCACQGAEIAJAYAQgAkBgBCACQGAEIAJAYAQgAkBgBCACQGAEIAJAYAQgAkBgBCACQGAEIAJAYAQgAkBgBCACQmLIKwI6OjpgzZ07U1NREQ0NDbN++/S2P/973vhcXXnhh1NTUxMUXXxydnZ1FmhQAoHTKJgC3bNkSra2t0dbWFjt37oy5c+dGc3NzvPbaa+Me/9xzz8W1114b119/fezatSuWLFkSS5YsiZ///OdFnhwAoLgqsizLSj3EZGhoaIjLLrssHnjggYiIGB0djXw+HzfffHOsXr36mONbWlriyJEj8YMf/KCw9pGPfCTmzZsXGzduPKH7HBwcjLq6uhgYGIja2trJeSAAwCnl+TtiSqkHmAzDw8OxY8eOWLNmTWGtsrIympqaoqenZ9xzenp6orW1dcxac3NzPPXUU8e9n6GhoRgaGir8PDAwEBH/9x8SAPDO8ObzdplcAzspZRGABw8ejJGRkaivrx+zXl9fH3v27Bn3nL6+vnGP7+vrO+79tLe3x1133XXMej6fP4mpAYBS+p//+Z+oq6sr9RglURYBWCxr1qwZc9Xw0KFD8d73vjf27duX7H9Ap4vBwcHI5/Oxf//+ZC/nny7sxenFfpw+7MXpY2BgIM4999x497vfXepRSqYsAnDatGlRVVUV/f39Y9b7+/tjxowZ454zY8aMCR0fEZHL5SKXyx2zXldX55f5NFFbW2svThP24vRiP04f9uL0UVlZNp+FnbCyeOTV1dUxf/786O7uLqyNjo5Gd3d3NDY2jntOY2PjmOMjIp555pnjHg8AUC7K4gpgRERra2ssX748FixYEAsXLowNGzbEkSNHYsWKFRERsWzZspg9e3a0t7dHRMQtt9wSV155Zdx3331xzTXXxObNm+NnP/tZPPTQQ6V8GAAAp1zZBGBLS0scOHAg1q5dG319fTFv3rzo6uoqfNBj3759Yy71Xn755fHYY4/FHXfcEbfddlv87d/+bTz11FNx0UUXnfB95nK5aGtrG/dlYYrLXpw+7MXpxX6cPuzF6cNelNH3AAIAcGLK4j2AAACcOAEIAJAYAQgAkBgBCACQGAH4F3R0dMScOXOipqYmGhoaYvv27W95/Pe+97248MILo6amJi6++OLo7Ows0qTlbyJ7sWnTpli0aFFMnTo1pk6dGk1NTX9x7zhxE/29eNPmzZujoqIilixZcmoHTMhE9+LQoUOxcuXKmDlzZuRyubjgggv8f2oSTXQ/NmzYEO9///vjzDPPjHw+H6tWrYo//vGPRZq2fP34xz+OxYsXx6xZs6KioiKeeuqpv3jOtm3b4sMf/nDkcrl43/veF48++ugpn7OkMo5r8+bNWXV1dfbII49kv/jFL7IbbrghO+ecc7L+/v5xj//JT36SVVVVZffcc0/2wgsvZHfccUd2xhlnZM8//3yRJy8/E92L6667Luvo6Mh27dqV7d69O/vUpz6V1dXVZf/93/9d5MnLz0T34k2vvPJKNnv27GzRokXZP/zDPxRn2DI30b0YGhrKFixYkF199dXZs88+m73yyivZtm3bst7e3iJPXp4muh/f+c53slwul33nO9/JXnnllezpp5/OZs6cma1atarIk5efzs7O7Pbbb8+eeOKJLCKyJ5988i2P37t3b3bWWWdlra2t2QsvvJB94xvfyKqqqrKurq7iDFwCAvAtLFy4MFu5cmXh55GRkWzWrFlZe3v7uMd/4hOfyK655poxaw0NDdlnPvOZUzpnCia6F3/u6NGj2dlnn519+9vfPlUjJuNk9uLo0aPZ5Zdfnn3rW9/Kli9fLgAnyUT34pvf/GZ23nnnZcPDw8UaMSkT3Y+VK1dmf/d3fzdmrbW1NbviiitO6ZypOZEA/OIXv5h96EMfGrPW0tKSNTc3n8LJSstLwMcxPDwcO3bsiKampsJaZWVlNDU1RU9Pz7jn9PT0jDk+IqK5ufm4x3NiTmYv/tzrr78eb7zxRtJ/8fdkONm9+PKXvxzTp0+P66+/vhhjJuFk9uL73/9+NDY2xsqVK6O+vj4uuuiiWLduXYyMjBRr7LJ1Mvtx+eWXx44dOwovE+/duzc6Ozvj6quvLsrM/EmKz99l8zeBTLaDBw/GyMhI4W8SeVN9fX3s2bNn3HP6+vrGPb6vr++UzZmCk9mLP3frrbfGrFmzjvkFZ2JOZi+effbZePjhh6O3t7cIE6bjZPZi79698V//9V/xyU9+Mjo7O+Pll1+Oz33uc/HGG29EW1tbMcYuWyezH9ddd10cPHgwPvrRj0aWZXH06NG46aab4rbbbivGyPw/x3v+HhwcjD/84Q9x5plnlmiyU8cVQMre+vXrY/PmzfHkk09GTU1NqcdJyuHDh2Pp0qWxadOmmDZtWqnHSd7o6GhMnz49HnrooZg/f360tLTE7bffHhs3biz1aEnatm1brFu3Lh588MHYuXNnPPHEE7F169a4++67Sz0aCXAF8DimTZsWVVVV0d/fP2a9v78/ZsyYMe45M2bMmNDxnJiT2Ys33XvvvbF+/fr44Q9/GJdccsmpHDMJE92LX/7yl/Hqq6/G4sWLC2ujo6MRETFlypR48cUX4/zzzz+1Q5epk/m9mDlzZpxxxhlRVVVVWPvABz4QfX19MTw8HNXV1ad05nJ2Mvtx5513xtKlS+PTn/50RERcfPHFceTIkbjxxhvj9ttvH/P313NqHe/5u7a2tiyv/kW4Anhc1dXVMX/+/Oju7i6sjY6ORnd3dzQ2No57TmNj45jjIyKeeeaZ4x7PiTmZvYiIuOeee+Luu++Orq6uWLBgQTFGLXsT3YsLL7wwnn/++ejt7S3cPv7xj8dVV10Vvb29kc/nizl+WTmZ34srrrgiXn755UKER0S89NJLMXPmTPH3Np3Mfrz++uvHRN6bcZ5l2akblmMk+fxd6k+hnM42b96c5XK57NFHH81eeOGF7MYbb8zOOeecrK+vL8uyLFu6dGm2evXqwvE/+clPsilTpmT33ntvtnv37qytrc3XwEySie7F+vXrs+rq6uzxxx/PfvOb3xRuhw8fLtVDKBsT3Ys/51PAk2eie7Fv377s7LPPzj7/+c9nL774YvaDH/wgmz59evaVr3ylVA+hrEx0P9ra2rKzzz47+/d///ds79692X/+539m559/fvaJT3yiVA+hbBw+fDjbtWtXtmvXriwisvvvvz/btWtX9qtf/SrLsixbvXp1tnTp0sLxb34NzBe+8IVs9+7dWUdHh6+BSd03vvGN7Nxzz82qq6uzhQsXZj/96U8L/+zKK6/Mli9fPub47373u9kFF1yQVVdXZx/60IeyrVu3Fnni8jWRvXjve9+bRcQxt7a2tuIPXoYm+nvx/wnAyTXRvXjuueeyhoaGLJfLZeedd1721a9+NTt69GiRpy5fE9mPN954I/vSl76UnX/++VlNTU2Wz+ezz33uc9nvfve74g9eZn70ox+N+xzw5r//5cuXZ1deeeUx58ybNy+rrq7OzjvvvOxf//Vfiz53MVVkmevMAAAp8R5AAIDECEAAgMQIQACAxAhAAIDECEAAgMQIQACAxAhAAIDECEAAgMQIQACAxAhAAIDECEAAgMQIQACAxAhAAIDECEAAgMQIQACAxAhAAIDECEAAgMQIQACAxAhAAIDECEAAgMQIQACAxAhAAIDECEAAgMQIQACAxAhAAIDECEAAgMQIQACAxAhAAIDECEAAgMQIQACAxAhAAIDE/C9EV9qKDRIt0wAAAABJRU5ErkJggg==' width=640.0/>\n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Assuming alpha_arr is a numpy array with shape [number_of_alphas, alpha_dimensions]\n",
    "# where each row is [alpha_0, alpha_1] for that epoch\n",
    "alpha_arr = np.asarray([alpha.detach().numpy() for alpha in clf.alpha_list])\n",
    "\n",
    "# Plot the trajectory of alpha values\n",
    "ax.plot(alpha_arr[:, 0], alpha_arr[:, 1], label='Alpha Trajectory')\n",
    "\n",
    "# Plot the starting point\n",
    "ax.plot(alpha_arr[0, 0], alpha_arr[0, 1], marker='o', label='Start')\n",
    "\n",
    "# Plot the ending point\n",
    "ax.plot(alpha_arr[-1, 0], alpha_arr[-1, 1], marker='*', label='End')\n",
    "\n",
    "if DATASET == 'synthetic':\n",
    "    ax.plot(alpha[0], alpha[1], marker='x', label='True Alpha')    \n",
    "\n",
    "# Add arrows to show direction\n",
    "for i in range(len(alpha_arr) - 1):\n",
    "    # Calculate the change in alpha\n",
    "    d_alpha = alpha_arr[i + 1] - alpha_arr[i]\n",
    "\n",
    "    # Add an arrow to the plot\n",
    "    ax.arrow(alpha_arr[i, 0], alpha_arr[i, 1], d_alpha[0], d_alpha[1],\n",
    "             head_width=0.02, head_length=0.03, fc='k', ec='k')\n",
    "\n",
    "# Optionally, set axis labels and a title\n",
    "ax.set_xlabel('Alpha 0')\n",
    "ax.set_ylabel('Alpha 1')\n",
    "ax.set_title('Alpha Trajectory with Direction')\n",
    "\n",
    "# Add a legend\n",
    "ax.legend()\n",
    "# ax.set_yscale('log')\n",
    "# ax.set_xscale('log')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma_list = [float(gamma) for gamma in clf.gamma_list]\n",
    "lambda_list = [float(lambda_) for lambda_ in clf.lambda_list]    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if clf.train_val_ratio:\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(clf.val_auc_list)\n",
    "    lowest_loss_index = clf.val_loss_list.index(min(clf.val_loss_list))\n",
    "    lowest_loss_auc = clf.val_auc_list[lowest_loss_index]\n",
    "    ax.plot(lowest_loss_index, lowest_loss_auc, marker='o', color='red')\n",
    "    # ax.annotate(f'AUC: {lowest_loss_index:.3f}', (lowest_loss_index, lowest_loss_auc), xytext=(lowest_loss_index, lowest_loss_auc + 0.05), color='red')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots()\n",
    "# ax.plot(lambda_list, label='Lambda')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the trajectory of alpha values\n",
    "# import torch\n",
    "gamma_lambda_arr = torch.cat([torch.tensor(clf.gamma_list).view(-1, 1), torch.tensor(clf.lambda_list).view(-1, 1), torch.zeros(len(clf.lambda_list)).view(-1, 1)], dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(gamma_lambda_arr[:, 0], label='Gamma')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Gamma')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Assuming alpha_arr is a numpy array with shape [number_of_alphas, alpha_dimensions]\n",
    "# where each row is [alpha_0, alpha_1] for that epoch\n",
    "gamma_lambda_arr = np.asarray(gamma_lambda_arr)\n",
    "\n",
    "# Plot the trajectory of alpha values\n",
    "plt.plot(gamma_lambda_arr[:, 0], gamma_lambda_arr[:, 1], label='Alpha Trajectory')\n",
    "\n",
    "# Plot the starting point\n",
    "plt.plot(gamma_lambda_arr[0, 0], gamma_lambda_arr[0, 1], marker='o', label='Start')\n",
    "\n",
    "# Plot the ending point\n",
    "plt.plot(gamma_lambda_arr[-1, 0], gamma_lambda_arr[-1, 1], marker='*', label='End')\n",
    "\n",
    "# plt.plot(true_gamma, true_lambda, marker='x', label='True Alpha')    \n",
    "# Add arrows to show direction\n",
    "for i in range(len(alpha_arr) - 1):\n",
    "    # Calculate the change in alpha\n",
    "    d_gamma_lambda = gamma_lambda_arr[i + 1] - gamma_lambda_arr[i]\n",
    "\n",
    "    # Add an arrow to the plot\n",
    "    ax.arrow(gamma_lambda_arr[i, 0], gamma_lambda_arr[i, 1], d_gamma_lambda[0], d_gamma_lambda[1],\n",
    "             head_width=0.002, head_length=0.004, fc='k', ec='k')\n",
    "\n",
    "# Optionally, set axis labels and a title\n",
    "ax.set_xlabel('Gamma')\n",
    "ax.set_ylabel('Lamnba')\n",
    "ax.set_title('Alpha Trajectory with Direction')\n",
    "\n",
    "# Add a legend\n",
    "ax.legend()\n",
    "# ax.set_yscale('log')\n",
    "# ax.set_xscale('log')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.exp(likelihood.gamma_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.softmax(torch.stack([likelihood.gamma, likelihood.lambda_, torch.zeros(1)]), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import LapPUAdapterTF\n",
    "# clf = LapPUAdapterTF.LapPUAdapterTF(encoder=encoder, \n",
    "#                                     gp_kernel_type='rbf', \n",
    "#                                     gp_kernel_amplitude=1., \n",
    "#                                     gp_kernel_lengthscale=1., \n",
    "#                                     estimator_type='logistic', \n",
    "#                                     scar_method='elkan', \n",
    "#                                     manifold_regularize=True, \n",
    "#                                     manifold_kernel_k=64, \n",
    "#                                     manifold_kernel_amplitude=2, \n",
    "#                                     manifold_kernel_lengthscale=.2, \n",
    "#                                     manifold_kernel_power=1, \n",
    "#                                     manifold_kernel_noise=0, \n",
    "#                                     manifold_neighbor_mode='distance')\n",
    "# clf.fit(sig_X_train, l_y_cat_transformed_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.utils.validation import check_is_fitted\n",
    "# check_is_fitted(encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from skopt import BayesSearchCV\n",
    "from  sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "import LapPUAdapterTF\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.metrics import make_scorer\n",
    "import logging\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "def custom_auc_score_function(y_true, y_proba):\n",
    "    # Compute AUC or similar metric using y_true and predicted probabilities y_proba\n",
    "    # For example:\n",
    "    l_true = (y_true // 2).astype(int)\n",
    "    # logging.warning(f\"y_true: {y_true}, l_true: {l_true}, y_proba: {y_proba}\")\n",
    "    # print (l_true, y_proba)\n",
    "    score = sklearn.metrics.roc_auc_score(l_true, y_proba)  # Assuming binary classification and y_proba is a 2D array\n",
    "    return score\n",
    "\n",
    "def custom_brier_score_function(y_true, y_proba):\n",
    "    # Compute AUC or similar metric using y_true and predicted probabilities y_proba\n",
    "    # For example:\n",
    "    l_true = (y_true // 2).astype(int)\n",
    "\n",
    "    # logging.warning(f\"y_true: {y_true}, l_true: {l_true}, y_proba: {y_proba}\")\n",
    "    # print (l_true, y_proba)\n",
    "    score = sklearn.metrics.brier_score_loss(l_true, y_proba)  # Assuming binary classification and y_proba is a 2D array\n",
    "    return score\n",
    "\n",
    "\n",
    "# Create a scorer object that knows it needs probabilities\n",
    "custom_auc_scorer = make_scorer(custom_auc_score_function, greater_is_better=True, needs_proba=True)\n",
    "custom_brier_scorer = make_scorer(custom_brier_score_function, greater_is_better=False, needs_proba=True)\n",
    "\n",
    "    \n",
    "search_spaces = {\n",
    "    'gp_kernel_type': Categorical([None]),\n",
    "    'gp_kernel_amplitude': Real(1e-2, 1e2, prior='log-uniform'),\n",
    "    # 'gp_kernel_lengthscale': Real(1e-2, 1e2, prior='log-uniform'),\n",
    "    'manifold_kernel_k': Categorical([16, 32, 64]),\n",
    "    'manifold_kernel_amplitude': Real(1e-2, 1e2, prior='log-uniform'),\n",
    "    'manifold_kernel_lengthscale': Real(1e-2, 1e2, prior='log-uniform'),\n",
    "    'manifold_neighbor_mode': Categorical(['distance']),\n",
    "    'manifold_regularize': Categorical([True, False]),\n",
    "    'scar_method': Categorical(['kme']),\n",
    "    'maxiter': Categorical([10000]),\n",
    "    'tol': Categorical([1e-8]),\n",
    "    # You don't need to include the encoder in the search space since it's pre-fitted\n",
    "}\n",
    "# grid_params = {\n",
    "#     'gp_kernel_amplitude': np.logspace(-2, 2, 5),\n",
    "#     # 'gp_kernel_lengthscale': np.logspace(-2, 2, 10),\n",
    "#     'scar_method': ['kme'], 'maxiter': [10000],\n",
    "#     'manifold_kernel_k' :[16, 32, 64],\n",
    "#     'manifold_kernel_amplitude': np.logspace(-2, 2, 5),\n",
    "#     'manifold_kernel_lengthscale': np.logspace(-2, 2, 5),\n",
    "#     'manifold_neighbor_mode': ['distance', 'connectivity'],\n",
    "#     'manifold_regularize': [True, False],\n",
    "#     'scar_method': ['kme'],\n",
    "#     'maxiter': [1000],\n",
    "#     # 'gp_kernel_type': ['linear'],\n",
    "#     'tol': [1e-8]}\n",
    "\n",
    "scar_clf = LapPUAdapterTF.LapPUAdapterTF(estimator_type='logistic',\n",
    "                                     calibrate=False,\n",
    "                                    )\n",
    "cv = StratifiedKFold(n_splits=5)\n",
    "\n",
    "opt = BayesSearchCV(estimator=scar_clf, search_spaces=search_spaces, n_iter=100, scoring=custom_brier_scorer, n_jobs=-1, cv=cv)\n",
    "# opt = GridSearchCV(estimator=scar_clf, param_grid=grid_params, scoring=custom_brier_scorer, n_jobs=-1, cv=cv)\n",
    "\n",
    "# Running the optimization\n",
    "opt.fit(sig_X_train, l_y_cat_transformed_train)  # No need to pre-transform labels, the pipeline will handle it\n",
    "\n",
    "# Best parameter set\n",
    "best_params = opt.best_params_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"hd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ELKAN METHOD NO MANIFOLD:\", GP_clf_sckre - sklearn.metrics.roc_auc_score(y_test, opt.best_estimator_.predict_prob_y_given_X(sig_X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OrderedDict([('gp_kernel_amplitude', 0.21700760945795225),\n",
    "#              ('gp_kernel_lengthscale', 6.5423196395182615),\n",
    "#              ('gp_kernel_type', None),\n",
    "#              ('manifold_kernel_amplitude', 10.0),\n",
    "#              ('manifold_kernel_k', 16),\n",
    "#              ('manifold_kernel_lengthscale', 0.1),\n",
    "#              ('manifold_neighbor_mode', 'distance'),\n",
    "#              ('manifold_regularize', True),\n",
    "#              ('maxiter', 10000),\n",
    "#              ('scar_method', 'kme')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.best_estimator_.estimator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.best_estimator_.scar_method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# For DistilBERT\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "model = AutoModel.from_pretrained(\"distilbert-base-uncased\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# # Example for BioBERT (replace \"bio-bert-model-name\" with the actual model name)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"bio-bert-model-name\")\n",
    "# model = AutoModel.from_pretrained(\"bio-bert-model-name\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
